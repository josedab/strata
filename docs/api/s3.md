# S3 API Compatibility Matrix

Strata provides an S3-compatible gateway that allows applications to interact with Strata using standard S3 SDKs and tools.

## Quick Start

```bash
# Configure AWS CLI to use Strata
aws configure set default.s3.endpoint_url http://localhost:9002

# Or use --endpoint-url flag
aws --endpoint-url http://localhost:9002 s3 ls
```

## Compatibility Overview

| Category | Support Level |
|----------|--------------|
| Bucket Operations | Full |
| Object Operations | Full |
| Multipart Upload | Full |
| Versioning | Supported |
| Lifecycle Policies | Supported |
| Server-Side Encryption | Supported |
| Object Locking (WORM) | Supported |
| Bucket Replication | Supported |
| Storage Classes | Supported |
| S3 Select | Supported |
| CORS | Supported |
| Access Control | AWS SigV4 |

## Supported Operations

### Service Operations

| Operation | Status | Notes |
|-----------|--------|-------|
| `ListBuckets` | ✅ Supported | Returns root-level directories as buckets |

### Bucket Operations

| Operation | Status | Notes |
|-----------|--------|-------|
| `CreateBucket` | ✅ Supported | Creates a directory at root level |
| `DeleteBucket` | ✅ Supported | Only succeeds if bucket is empty |
| `HeadBucket` | ✅ Supported | Checks bucket existence |
| `ListObjects` | ✅ Supported | Supports prefix and max-keys |
| `ListObjectsV2` | ✅ Supported | Same as ListObjects |
| `GetBucketVersioning` | ✅ Supported | Get versioning configuration |
| `PutBucketVersioning` | ✅ Supported | Enable/suspend versioning |
| `GetBucketCors` | ✅ Supported | Get CORS configuration |
| `PutBucketCors` | ✅ Supported | Set CORS rules |
| `DeleteBucketCors` | ✅ Supported | Remove CORS configuration |
| `GetBucketLifecycle` | ✅ Supported | Get lifecycle rules |
| `PutBucketLifecycle` | ✅ Supported | Set lifecycle rules |
| `DeleteBucketLifecycle` | ✅ Supported | Remove lifecycle configuration |
| `GetBucketEncryption` | ✅ Supported | Get encryption configuration |
| `PutBucketEncryption` | ✅ Supported | Set default encryption |
| `DeleteBucketEncryption` | ✅ Supported | Remove encryption configuration |
| `GetObjectLockConfiguration` | ✅ Supported | Get WORM configuration |
| `PutObjectLockConfiguration` | ✅ Supported | Enable object locking |
| `GetBucketReplication` | ✅ Supported | Get replication rules |
| `PutBucketReplication` | ✅ Supported | Configure cross-region replication |
| `DeleteBucketReplication` | ✅ Supported | Remove replication configuration |
| `GetBucketLocation` | ❌ Not Supported | - |
| `GetBucketPolicy` | ❌ Not Supported | - |
| `PutBucketPolicy` | ❌ Not Supported | - |

### Object Operations

| Operation | Status | Notes |
|-----------|--------|-------|
| `PutObject` | ✅ Supported | Creates/updates objects, stores data in chunks, returns MD5 ETag |
| `GetObject` | ✅ Supported | Returns full object data from chunk storage |
| `HeadObject` | ✅ Supported | Returns object metadata |
| `DeleteObject` | ✅ Supported | Returns success even if object doesn't exist (per S3 spec) |
| `SelectObjectContent` | ✅ Supported | SQL queries on CSV/JSON objects |
| `RestoreObject` | ✅ Supported | Restore from GLACIER/DEEP_ARCHIVE |
| `GetObjectRetention` | ✅ Supported | Get retention settings |
| `PutObjectRetention` | ✅ Supported | Set retention period |
| `GetObjectLegalHold` | ✅ Supported | Get legal hold status |
| `PutObjectLegalHold` | ✅ Supported | Enable/disable legal hold |
| `CopyObject` | ❌ Not Supported | - |
| `GetObjectAcl` | ❌ Not Supported | - |
| `PutObjectAcl` | ❌ Not Supported | - |

### Multipart Upload Operations

| Operation | Status | Notes |
|-----------|--------|-------|
| `CreateMultipartUpload` | ✅ Supported | Returns upload ID for multipart upload |
| `UploadPart` | ✅ Supported | Uploads individual parts (1-10000) |
| `CompleteMultipartUpload` | ✅ Supported | Completes upload, assembles file from parts |
| `AbortMultipartUpload` | ✅ Supported | Cancels upload, cleans up parts |
| `ListParts` | ❌ Not Supported | - |
| `ListMultipartUploads` | ❌ Not Supported | - |

---

## Feature Details

### Versioning

Enable object versioning to preserve, retrieve, and restore every version of every object stored in a bucket.

```bash
# Enable versioning
aws --endpoint-url http://localhost:9002 s3api put-bucket-versioning \
    --bucket mybucket \
    --versioning-configuration Status=Enabled

# Get versioning status
aws --endpoint-url http://localhost:9002 s3api get-bucket-versioning \
    --bucket mybucket
```

**Configuration XML:**
```xml
<?xml version="1.0" encoding="UTF-8"?>
<VersioningConfiguration>
    <Status>Enabled</Status>
</VersioningConfiguration>
```

**Versioning States:**
| Status | Description |
|--------|-------------|
| `Enabled` | New object versions created for all changes |
| `Suspended` | No new versions; existing versions preserved |

---

### Lifecycle Policies

Automate object management with lifecycle rules for expiration and storage class transitions.

```bash
# Set lifecycle configuration
aws --endpoint-url http://localhost:9002 s3api put-bucket-lifecycle-configuration \
    --bucket mybucket \
    --lifecycle-configuration file://lifecycle.json
```

**Example lifecycle.json:**
```json
{
    "Rules": [
        {
            "ID": "archive-old-logs",
            "Status": "Enabled",
            "Filter": {
                "Prefix": "logs/"
            },
            "Transitions": [
                {
                    "Days": 30,
                    "StorageClass": "GLACIER"
                }
            ],
            "Expiration": {
                "Days": 365
            }
        }
    ]
}
```

**Configuration XML:**
```xml
<?xml version="1.0" encoding="UTF-8"?>
<LifecycleConfiguration>
    <Rule>
        <ID>archive-old-logs</ID>
        <Status>Enabled</Status>
        <Filter>
            <Prefix>logs/</Prefix>
        </Filter>
        <Transition>
            <Days>30</Days>
            <StorageClass>GLACIER</StorageClass>
        </Transition>
        <Expiration>
            <Days>365</Days>
        </Expiration>
    </Rule>
</LifecycleConfiguration>
```

**Supported Storage Classes:**
| Class | Description |
|-------|-------------|
| `STANDARD` | Default, frequently accessed data |
| `STANDARD_IA` | Infrequent access, lower cost |
| `GLACIER` | Archive storage, retrieval required |
| `DEEP_ARCHIVE` | Long-term archive, slowest retrieval |

---

### Server-Side Encryption

Protect data at rest with server-side encryption.

```bash
# Set default bucket encryption
aws --endpoint-url http://localhost:9002 s3api put-bucket-encryption \
    --bucket mybucket \
    --server-side-encryption-configuration '{
        "Rules": [{
            "ApplyServerSideEncryptionByDefault": {
                "SSEAlgorithm": "AES256"
            }
        }]
    }'

# Upload with encryption headers
aws --endpoint-url http://localhost:9002 s3 cp myfile.txt s3://mybucket/ \
    --sse AES256
```

**Configuration XML:**
```xml
<?xml version="1.0" encoding="UTF-8"?>
<ServerSideEncryptionConfiguration>
    <Rule>
        <ApplyServerSideEncryptionByDefault>
            <SSEAlgorithm>AES256</SSEAlgorithm>
        </ApplyServerSideEncryptionByDefault>
    </Rule>
</ServerSideEncryptionConfiguration>
```

**Supported Algorithms:**
| Algorithm | Description |
|-----------|-------------|
| `AES256` | AES-256-GCM encryption (SSE-S3) |
| `aws:kms` | KMS-managed keys (SSE-KMS) |

**Request Headers:**
| Header | Description |
|--------|-------------|
| `x-amz-server-side-encryption` | Encryption algorithm to use |
| `x-amz-server-side-encryption-aws-kms-key-id` | KMS key ID (for SSE-KMS) |

---

### Object Locking (WORM Compliance)

Enforce write-once-read-many (WORM) protection for compliance requirements.

```bash
# Enable object lock on bucket (must be set at bucket creation)
aws --endpoint-url http://localhost:9002 s3api put-object-lock-configuration \
    --bucket mybucket \
    --object-lock-configuration '{
        "ObjectLockEnabled": "Enabled",
        "Rule": {
            "DefaultRetention": {
                "Mode": "GOVERNANCE",
                "Days": 30
            }
        }
    }'

# Set legal hold on an object
aws --endpoint-url http://localhost:9002 s3api put-object-legal-hold \
    --bucket mybucket \
    --key important-doc.pdf \
    --legal-hold Status=ON

# Set retention on an object
aws --endpoint-url http://localhost:9002 s3api put-object-retention \
    --bucket mybucket \
    --key important-doc.pdf \
    --retention '{
        "Mode": "COMPLIANCE",
        "RetainUntilDate": "2025-12-31T00:00:00Z"
    }'
```

**Retention Modes:**
| Mode | Description |
|------|-------------|
| `GOVERNANCE` | Protected but can be overridden with special permissions |
| `COMPLIANCE` | Cannot be shortened or disabled by any user |

**Legal Hold:**
- Independent of retention periods
- Prevents object deletion while active
- Can be toggled on/off by authorized users

---

### Bucket Replication

Automatically replicate objects to a destination bucket for disaster recovery or compliance.

```bash
# Configure replication
aws --endpoint-url http://localhost:9002 s3api put-bucket-replication \
    --bucket mybucket \
    --replication-configuration file://replication.json
```

**Example replication.json:**
```json
{
    "Role": "arn:aws:iam::123456789012:role/replication-role",
    "Rules": [
        {
            "ID": "replicate-all",
            "Status": "Enabled",
            "Priority": 1,
            "Filter": {},
            "Destination": {
                "Bucket": "arn:aws:s3:::backup-bucket",
                "StorageClass": "STANDARD"
            }
        }
    ]
}
```

**Configuration XML:**
```xml
<?xml version="1.0" encoding="UTF-8"?>
<ReplicationConfiguration>
    <Role>arn:aws:iam::123456789012:role/replication-role</Role>
    <Rule>
        <ID>replicate-all</ID>
        <Status>Enabled</Status>
        <Priority>1</Priority>
        <Filter></Filter>
        <Destination>
            <Bucket>arn:aws:s3:::backup-bucket</Bucket>
            <StorageClass>STANDARD</StorageClass>
        </Destination>
    </Rule>
</ReplicationConfiguration>
```

**Replication Statuses:**
| Status | Description |
|--------|-------------|
| `PENDING` | Object queued for replication |
| `COMPLETED` | Successfully replicated |
| `FAILED` | Replication failed |
| `REPLICA` | Object is a replica |

---

### Storage Class Transitions & Restore

Move objects between storage classes and restore archived objects.

```bash
# Restore an object from GLACIER
aws --endpoint-url http://localhost:9002 s3api restore-object \
    --bucket mybucket \
    --key archived-file.zip \
    --restore-request '{
        "Days": 7,
        "GlacierJobParameters": {
            "Tier": "Standard"
        }
    }'

# Check restore status (via HEAD request)
aws --endpoint-url http://localhost:9002 s3api head-object \
    --bucket mybucket \
    --key archived-file.zip
```

**Restore Tiers:**
| Tier | Typical Time | Use Case |
|------|--------------|----------|
| `Expedited` | 1-5 minutes | Urgent access |
| `Standard` | 3-5 hours | Normal retrieval |
| `Bulk` | 5-12 hours | Large-scale, cost-effective |

**Restore Request XML:**
```xml
<?xml version="1.0" encoding="UTF-8"?>
<RestoreRequest>
    <Days>7</Days>
    <GlacierJobParameters>
        <Tier>Standard</Tier>
    </GlacierJobParameters>
</RestoreRequest>
```

---

### S3 Select

Query data in place using SQL expressions without downloading the entire object.

```bash
# Query a CSV file
aws --endpoint-url http://localhost:9002 s3api select-object-content \
    --bucket mybucket \
    --key data.csv \
    --expression "SELECT name, age FROM s3object WHERE age > 30" \
    --expression-type SQL \
    --input-serialization '{"CSV": {"FileHeaderInfo": "USE"}}' \
    --output-serialization '{"CSV": {}}' \
    output.csv
```

**Supported SQL:**
```sql
-- Basic SELECT
SELECT * FROM s3object

-- Column selection
SELECT col1, col2, col3 FROM s3object

-- Filtering
SELECT * FROM s3object WHERE age > 25

-- Aggregate functions
SELECT COUNT(*), SUM(amount), AVG(price) FROM s3object

-- Combined
SELECT department, AVG(salary) FROM s3object WHERE status = 'active'
```

**Request XML:**
```xml
<?xml version="1.0" encoding="UTF-8"?>
<SelectObjectContentRequest>
    <Expression>SELECT * FROM s3object WHERE price > 100</Expression>
    <ExpressionType>SQL</ExpressionType>
    <InputSerialization>
        <CSV>
            <FileHeaderInfo>USE</FileHeaderInfo>
            <FieldDelimiter>,</FieldDelimiter>
        </CSV>
    </InputSerialization>
    <OutputSerialization>
        <CSV>
            <FieldDelimiter>,</FieldDelimiter>
        </CSV>
    </OutputSerialization>
</SelectObjectContentRequest>
```

**Supported Features:**
| Feature | Support |
|---------|---------|
| CSV input | ✅ Supported |
| JSON input | ✅ Supported |
| Parquet input | ❌ Not Supported |
| Aggregate functions | ✅ COUNT, SUM, AVG, MIN, MAX |
| WHERE clause | ✅ =, !=, <, >, <=, >= |
| LIMIT clause | ❌ Not Supported |
| GROUP BY | ❌ Not Supported |

---

### CORS Configuration

Enable cross-origin resource sharing for browser-based applications.

```bash
# Set CORS configuration
aws --endpoint-url http://localhost:9002 s3api put-bucket-cors \
    --bucket mybucket \
    --cors-configuration file://cors.json
```

**Example cors.json:**
```json
{
    "CORSRules": [
        {
            "AllowedOrigins": ["https://example.com"],
            "AllowedMethods": ["GET", "PUT", "POST"],
            "AllowedHeaders": ["*"],
            "ExposeHeaders": ["ETag", "x-amz-meta-custom"],
            "MaxAgeSeconds": 3600
        }
    ]
}
```

**Configuration XML:**
```xml
<?xml version="1.0" encoding="UTF-8"?>
<CORSConfiguration>
    <CORSRule>
        <AllowedOrigin>https://example.com</AllowedOrigin>
        <AllowedMethod>GET</AllowedMethod>
        <AllowedMethod>PUT</AllowedMethod>
        <AllowedMethod>POST</AllowedMethod>
        <AllowedHeader>*</AllowedHeader>
        <ExposeHeader>ETag</ExposeHeader>
        <MaxAgeSeconds>3600</MaxAgeSeconds>
        <AllowCredentials>true</AllowCredentials>
    </CORSRule>
</CORSConfiguration>
```

**CORS with Credentials:**
- When `AllowCredentials` is true, `AllowedOrigin` cannot be `*`
- Must specify explicit origins for credential-based requests
- Response includes `Access-Control-Allow-Credentials: true`

**CORS Response Headers:**
| Header | Description |
|--------|-------------|
| `Access-Control-Allow-Origin` | Allowed origin |
| `Access-Control-Allow-Methods` | Allowed HTTP methods |
| `Access-Control-Allow-Headers` | Allowed request headers |
| `Access-Control-Expose-Headers` | Headers exposed to browser |
| `Access-Control-Max-Age` | Preflight cache duration |
| `Access-Control-Allow-Credentials` | Whether credentials allowed |

---

## Authentication

Strata supports AWS Signature Version 4 (SigV4) authentication.

### Configuration

```toml
[s3.auth]
allow_anonymous = false
credentials = [
    { access_key_id = "AKIAIOSFODNN7EXAMPLE", secret_key = "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY", user_id = "admin", display_name = "Admin User" }
]
```

### Settings

| Setting | Default | Description |
|---------|---------|-------------|
| `allow_anonymous` | `true` | Allow unsigned requests |
| `credentials` | `[]` | List of access key credentials |

### Timestamp Validation

To prevent replay attacks, Strata validates request timestamps:
- Requests must include `x-amz-date` header (SigV4)
- Request timestamp must be within ±15 minutes of server time
- Expired requests return `RequestTimeTooSkewed`

---

## Rate Limiting

Strata implements per-client rate limiting to protect against abuse.

### Default Limits

| Limit Type | Default | Description |
|------------|---------|-------------|
| Requests per second | 100 | Maximum sustained request rate per client |
| Burst capacity | 200 | Maximum burst requests allowed |
| Global limit | 10000 | Maximum requests per second across all clients |

### Rate Limit Response

When rate limits are exceeded:

**Client limit exceeded (HTTP 429):**
```xml
<?xml version="1.0" encoding="UTF-8"?>
<Error>
    <Code>SlowDown</Code>
    <Message>Rate limit exceeded. Please reduce your request rate.</Message>
</Error>
```

**Global limit exceeded (HTTP 503):**
```xml
<?xml version="1.0" encoding="UTF-8"?>
<Error>
    <Code>SlowDown</Code>
    <Message>Rate limit exceeded. Please reduce your request rate.</Message>
</Error>
```

The response includes a `Retry-After` header indicating when to retry.

---

## Security

### Request Validation

| Validation | Limit | Error |
|------------|-------|-------|
| Key length | Max 1024 characters | `InvalidRequest` |
| Bucket name length | Max 63 characters | `InvalidRequest` |
| Path traversal | `..` sequences blocked | `InvalidRequest` |
| Null bytes | Not allowed in paths | `InvalidRequest` |

### Best Practices

1. **Use HTTPS**: Always use TLS in production
2. **Rotate credentials**: Periodically rotate access keys
3. **Enable object locking**: For compliance-sensitive data
4. **Configure CORS carefully**: Limit allowed origins
5. **Enable encryption**: Use SSE for sensitive data

---

## Error Responses

Strata returns standard S3 XML error responses:

```xml
<?xml version="1.0" encoding="UTF-8"?>
<Error>
    <Code>NoSuchBucket</Code>
    <Message>The specified bucket does not exist</Message>
    <BucketName>mybucket</BucketName>
</Error>
```

### Error Codes

| Error Code | HTTP Status | Description |
|------------|-------------|-------------|
| `AccessDenied` | 403 | Authentication failed or not provided |
| `InvalidRequest` | 400 | Malformed request or invalid parameters |
| `RequestTimeTooSkewed` | 403 | Request timestamp too far from server time |
| `SlowDown` | 429/503 | Rate limit exceeded |
| `NoSuchBucket` | 404 | Bucket does not exist |
| `NoSuchKey` | 404 | Object does not exist |
| `BucketAlreadyExists` | 409 | Bucket already exists |
| `BucketNotEmpty` | 409 | Cannot delete non-empty bucket |
| `ObjectLocked` | 403 | Object is locked (WORM) |
| `InvalidRetentionPeriod` | 400 | Invalid retention configuration |
| `InternalError` | 500 | Server-side error |

---

## Usage Examples

### AWS CLI

```bash
# List all buckets
aws --endpoint-url http://localhost:9002 s3 ls

# Create a bucket
aws --endpoint-url http://localhost:9002 s3 mb s3://mybucket

# Upload a file
aws --endpoint-url http://localhost:9002 s3 cp myfile.txt s3://mybucket/

# Upload with encryption
aws --endpoint-url http://localhost:9002 s3 cp myfile.txt s3://mybucket/ --sse AES256

# List bucket contents
aws --endpoint-url http://localhost:9002 s3 ls s3://mybucket/

# Download a file
aws --endpoint-url http://localhost:9002 s3 cp s3://mybucket/myfile.txt ./

# Delete an object
aws --endpoint-url http://localhost:9002 s3 rm s3://mybucket/myfile.txt

# Sync a directory
aws --endpoint-url http://localhost:9002 s3 sync ./data s3://mybucket/data/
```

### AWS SDK (Python/Boto3)

```python
import boto3

# Create S3 client
s3 = boto3.client(
    's3',
    endpoint_url='http://localhost:9002',
    aws_access_key_id='AKIAIOSFODNN7EXAMPLE',
    aws_secret_access_key='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY',
    region_name='us-east-1'
)

# List buckets
response = s3.list_buckets()
for bucket in response['Buckets']:
    print(bucket['Name'])

# Create bucket with object locking
s3.create_bucket(
    Bucket='secure-bucket',
    ObjectLockEnabledForBucket=True
)

# Upload object with encryption
s3.put_object(
    Bucket='mybucket',
    Key='sensitive.txt',
    Body=b'Sensitive data',
    ServerSideEncryption='AES256'
)

# Set legal hold
s3.put_object_legal_hold(
    Bucket='mybucket',
    Key='important.pdf',
    LegalHold={'Status': 'ON'}
)

# Query CSV with S3 Select
response = s3.select_object_content(
    Bucket='mybucket',
    Key='data.csv',
    ExpressionType='SQL',
    Expression='SELECT * FROM s3object WHERE price > 100',
    InputSerialization={'CSV': {'FileHeaderInfo': 'USE'}},
    OutputSerialization={'CSV': {}}
)
```

### AWS SDK (JavaScript/Node.js)

```javascript
const { S3Client, PutObjectCommand, SelectObjectContentCommand } = require('@aws-sdk/client-s3');

const client = new S3Client({
    endpoint: 'http://localhost:9002',
    region: 'us-east-1',
    credentials: {
        accessKeyId: 'AKIAIOSFODNN7EXAMPLE',
        secretAccessKey: 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'
    },
    forcePathStyle: true
});

// Upload with encryption
await client.send(new PutObjectCommand({
    Bucket: 'mybucket',
    Key: 'hello.txt',
    Body: 'Hello, Strata!',
    ServerSideEncryption: 'AES256'
}));

// S3 Select query
const response = await client.send(new SelectObjectContentCommand({
    Bucket: 'mybucket',
    Key: 'data.csv',
    ExpressionType: 'SQL',
    Expression: 'SELECT name, age FROM s3object WHERE age > 30',
    InputSerialization: { CSV: { FileHeaderInfo: 'USE' } },
    OutputSerialization: { CSV: {} }
}));
```

---

## Known Limitations

### Current Limitations

1. **No ListParts/ListMultipartUploads**: Cannot list in-progress multipart uploads
2. **No CopyObject**: Server-side copy not implemented
3. **No Presigned URLs**: Not yet implemented
4. **No IAM Policies**: Only access key validation, no bucket/object-level policies
5. **No S3 Batch Operations**: Bulk operations not supported
6. **Limited S3 Select**: No Parquet, GROUP BY, or LIMIT support

### Workarounds

| Limitation | Workaround |
|------------|------------|
| CopyObject | Download and re-upload via client |
| Presigned URLs | Use direct authenticated requests |
| Fine-grained access | Use separate credentials per user/application |

---

## Configuration Reference

### S3 Gateway Settings

```toml
[s3]
enabled = true
bind_addr = "0.0.0.0:9002"
region = "us-east-1"

[s3.auth]
allow_anonymous = false
credentials = [
    { access_key_id = "...", secret_key = "...", user_id = "admin" }
]

[s3.rate_limit]
enabled = true
requests_per_second = 100
burst_size = 200
global_requests_per_second = 10000
```

### Environment Variables

```bash
export STRATA_S3_ENABLED=true
export STRATA_S3_BIND_ADDR="0.0.0.0:9002"
export STRATA_S3_REGION="us-east-1"
export STRATA_S3_AUTH_ALLOW_ANONYMOUS=false
```

---

## See Also

- [Quick Start Guide](../guides/quickstart.md) - Getting started with S3
- [Architecture Overview](../architecture/overview.md) - S3 gateway architecture
- [Configuration Reference](../guides/configuration.md) - S3 configuration options
